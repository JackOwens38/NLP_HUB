{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Concepts using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ganad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ganad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "from nltk.tokenize import (word_tokenize,\n",
    "                           sent_tokenize,\n",
    "                           TreebankWordTokenizer,\n",
    "                           wordpunct_tokenize,\n",
    "                           TweetTokenizer,\n",
    "                           MWETokenizer)\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a more complex sentence to showcase tokenization\n",
    "complex_sentence = \"\"\"Dr. Emily Wong, Ph.D., exclaimed, 'OMG! Despite reaching 7.8 billion in 2021, global population metrics, like those in Section 123(i)(3)(A)(ii), remain baffling,' and then she added in a mix of French and English, 'C’est incroyable, no? Check my blog at www.emilywong-science.com or email me at wong@scienceworld.com for the full story on Mycobacterium tuberculosis complex (MTBC) research, which is, you know, the state-of-the-art stuff—I've literally termed it \"the Occam’s razor of epidemiology.\"\"\"\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['The quick brown fox jumps over the lazy dog.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokens_word = nltk.word_tokenize(sentence)\n",
    "token_sent = nltk.sent_tokenize(sentence)\n",
    "token_treebank = TreebankWordTokenizer().tokenize(sentence)\n",
    "token_wordpunct = wordpunct_tokenize(sentence)\n",
    "token_tweet = TweetTokenizer().tokenize(sentence)\n",
    "token_mwe = MWETokenizer().tokenize(sentence.split())\n",
    "\n",
    "\n",
    "display(tokens_word,\n",
    "        token_sent,\n",
    "        token_treebank,\n",
    "        token_wordpunct,\n",
    "        token_tweet,\n",
    "        token_mwe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dr.', 'Emily', 'Wong', ',', 'Ph.D.', ',', 'exclaimed', ',', \"'OMG\", '!', 'Despite', 'reaching', '7.8', 'billion', 'in', '2021', ',', 'global', 'population', 'metrics', ',', 'like', 'those', 'in', 'Section', '123', '(', 'i', ')', '(', '3', ')', '(', 'A', ')', '(', 'ii', ')', ',', 'remain', 'baffling', ',', \"'\", 'and', 'then', 'she', 'added', 'in', 'a', 'mix', 'of', 'French', 'and', 'English', ',', \"'\", 'C', '’', 'est', 'incroyable', ',', 'no', '?', 'Check', 'my', 'blog', 'at', 'www.emilywong-science.com', 'or', 'email', 'me', 'at', 'wong', '@', 'scienceworld.com', 'for', 'the', 'full', 'story', 'on', 'Mycobacterium', 'tuberculosis', 'complex', '(', 'MTBC', ')', 'research', ',', 'which', 'is', ',', 'you', 'know', ',', 'the', 'state-of-the-art', 'stuff—I', \"'ve\", 'literally', 'termed', 'it', '``', 'the', 'Occam', '’', 's', 'razor', 'of', 'epidemiology', '.']\n",
      "\n",
      "[\"Dr. Emily Wong, Ph.D., exclaimed, 'OMG!\", \"Despite reaching 7.8 billion in 2021, global population metrics, like those in Section 123(i)(3)(A)(ii), remain baffling,' and then she added in a mix of French and English, 'C’est incroyable, no?\", 'Check my blog at www.emilywong-science.com or email me at wong@scienceworld.com for the full story on Mycobacterium tuberculosis complex (MTBC) research, which is, you know, the state-of-the-art stuff—I\\'ve literally termed it \"the Occam’s razor of epidemiology.']\n",
      "\n",
      "['Dr.', 'Emily', 'Wong', ',', 'Ph.D.', ',', 'exclaimed', ',', \"'OMG\", '!', 'Despite', 'reaching', '7.8', 'billion', 'in', '2021', ',', 'global', 'population', 'metrics', ',', 'like', 'those', 'in', 'Section', '123', '(', 'i', ')', '(', '3', ')', '(', 'A', ')', '(', 'ii', ')', ',', 'remain', 'baffling', ',', \"'\", 'and', 'then', 'she', 'added', 'in', 'a', 'mix', 'of', 'French', 'and', 'English', ',', \"'C’est\", 'incroyable', ',', 'no', '?', 'Check', 'my', 'blog', 'at', 'www.emilywong-science.com', 'or', 'email', 'me', 'at', 'wong', '@', 'scienceworld.com', 'for', 'the', 'full', 'story', 'on', 'Mycobacterium', 'tuberculosis', 'complex', '(', 'MTBC', ')', 'research', ',', 'which', 'is', ',', 'you', 'know', ',', 'the', 'state-of-the-art', 'stuff—I', \"'ve\", 'literally', 'termed', 'it', '``', 'the', 'Occam’s', 'razor', 'of', 'epidemiology', '.']\n",
      "\n",
      "['Dr', '.', 'Emily', 'Wong', ',', 'Ph', '.', 'D', '.,', 'exclaimed', ',', \"'\", 'OMG', '!', 'Despite', 'reaching', '7', '.', '8', 'billion', 'in', '2021', ',', 'global', 'population', 'metrics', ',', 'like', 'those', 'in', 'Section', '123', '(', 'i', ')(', '3', ')(', 'A', ')(', 'ii', '),', 'remain', 'baffling', \",'\", 'and', 'then', 'she', 'added', 'in', 'a', 'mix', 'of', 'French', 'and', 'English', ',', \"'\", 'C', '’', 'est', 'incroyable', ',', 'no', '?', 'Check', 'my', 'blog', 'at', 'www', '.', 'emilywong', '-', 'science', '.', 'com', 'or', 'email', 'me', 'at', 'wong', '@', 'scienceworld', '.', 'com', 'for', 'the', 'full', 'story', 'on', 'Mycobacterium', 'tuberculosis', 'complex', '(', 'MTBC', ')', 'research', ',', 'which', 'is', ',', 'you', 'know', ',', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'stuff', '—', 'I', \"'\", 've', 'literally', 'termed', 'it', '\"', 'the', 'Occam', '’', 's', 'razor', 'of', 'epidemiology', '.']\n",
      "\n",
      "['Dr', '.', 'Emily', 'Wong', ',', 'Ph', '.', 'D', '.', ',', 'exclaimed', ',', \"'\", 'OMG', '!', 'Despite', 'reaching', '7.8', 'billion', 'in', '2021', ',', 'global', 'population', 'metrics', ',', 'like', 'those', 'in', 'Section', '123', '(', 'i', ')', '(', '3', ')', '(', 'A', ')', '(', 'ii', ')', ',', 'remain', 'baffling', ',', \"'\", 'and', 'then', 'she', 'added', 'in', 'a', 'mix', 'of', 'French', 'and', 'English', ',', \"'\", 'C', '’', 'est', 'incroyable', ',', 'no', '?', 'Check', 'my', 'blog', 'at', 'www.emilywong-science.com', 'or', 'email', 'me', 'at', 'wong@scienceworld.com', 'for', 'the', 'full', 'story', 'on', 'Mycobacterium', 'tuberculosis', 'complex', '(', 'MTBC', ')', 'research', ',', 'which', 'is', ',', 'you', 'know', ',', 'the', 'state-of-the-art', 'stuff', '—', \"I've\", 'literally', 'termed', 'it', '\"', 'the', 'Occam', '’', 's', 'razor', 'of', 'epidemiology', '.']\n",
      "\n",
      "['Dr.', 'Emily', 'Wong,', 'Ph.D.,', 'exclaimed,', \"'OMG!\", 'Despite', 'reaching', '7.8', 'billion', 'in', '2021,', 'global', 'population', 'metrics,', 'like', 'those', 'in', 'Section', '123(i)(3)(A)(ii),', 'remain', \"baffling,'\", 'and', 'then', 'she', 'added', 'in', 'a', 'mix', 'of', 'French', 'and', 'English,', \"'C’est\", 'incroyable,', 'no?', 'Check', 'my', 'blog', 'at', 'www.emilywong-science.com', 'or', 'email', 'me', 'at', 'wong@scienceworld.com', 'for', 'the', 'full', 'story', 'on', 'Mycobacterium', 'tuberculosis', 'complex', '(MTBC)', 'research,', 'which', 'is,', 'you', 'know,', 'the', 'state-of-the-art', \"stuff—I've\", 'literally', 'termed', 'it', '\"the', 'Occam’s', 'razor', 'of', 'epidemiology.']\n"
     ]
    }
   ],
   "source": [
    "tokens_word_c = nltk.word_tokenize(complex_sentence)\n",
    "token_sent_c = nltk.sent_tokenize(complex_sentence)\n",
    "token_treebank_c = TreebankWordTokenizer().tokenize(complex_sentence)\n",
    "token_wordpunct_c = wordpunct_tokenize(complex_sentence)\n",
    "token_tweet_c = TweetTokenizer().tokenize(complex_sentence)\n",
    "token_mwe_c = MWETokenizer().tokenize(complex_sentence.split())\n",
    "\n",
    "print(tokens_word_c)\n",
    "print()\n",
    "print(token_sent_c)\n",
    "print()\n",
    "print(token_treebank_c)\n",
    "print()\n",
    "print(token_wordpunct_c)\n",
    "print()\n",
    "print(token_tweet_c)\n",
    "print()\n",
    "print(token_mwe_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "- Word Tokenizer and the Treebank Tokenizer seems pretty good compared to the others\n",
    "\n",
    "- Picking the right tokenizer depends on the task at hand. For example, if you are working on a sentiment analysis task, you might want to use a tokenizer that preserves emoticons and hashtags. If you are working on a machine translation task, you might want to use a tokenizer that preserves punctuation and capitalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penn Treebank Tagset Overview\n",
    "\n",
    "A tagset is a list of part-of-speech tags, i.e., labels used to indicate the part of speech and often also other grammatical categories (case, tense etc.) of each token in a text corpus.\n",
    "\n",
    "#### Introduction to Penn Treebank Tagset\n",
    "\n",
    "The English Penn Treebank tagset is utilized with English corpora annotated by the TreeTagger tool, developed by Helmut Schmid in the TC project at the Institute for Computational Linguistics of the University of Stuttgart. This version of the tagset contains modifications developed by Sketch Engine (earlier version).\n",
    "\n",
    "[See a more recent version of this tagset](https://www.sketchengine.eu/penn-treebank-tagset/).\n",
    "\n",
    "### What is a POS Tag?\n",
    "\n",
    "POS tags classify words into grammatical categories which can help in understanding the structure and context of text. The table below shows the English Penn TreeBank tagset with Sketch Engine modifications (earlier version).\n",
    "\n",
    "Example: Using `[tag=\"NNS\"]` finds all nouns in the plural, e.g., people, years when used in the CQL concordance search (always use straight double quotation marks in CQL).\n",
    "\n",
    "| POS Tag | Description                                   | Example             |\n",
    "|---------|-----------------------------------------------|---------------------|\n",
    "| CC      | Coordinating conjunction                      | and                 |\n",
    "| CD      | Cardinal number                               | 1, third            |\n",
    "| DT      | Determiner                                    | the                 |\n",
    "| EX      | Existential there                             | there is            |\n",
    "| FW      | Foreign word                                  | les                 |\n",
    "| IN      | Preposition, subordinating conjunction        | in, of, like        |\n",
    "| IN/that | That as subordinator                          | that                |\n",
    "| JJ      | Adjective                                     | green               |\n",
    "| JJR     | Adjective, comparative                        | greener             |\n",
    "| JJS     | Adjective, superlative                        | greenest            |\n",
    "| LS      | List marker                                   | 1)                  |\n",
    "| MD      | Modal                                         | could, will         |\n",
    "| NN      | Noun, singular or mass                        | table               |\n",
    "| NNS     | Noun, plural                                  | tables              |\n",
    "| NP      | Proper noun, singular                         | John                |\n",
    "| NPS     | Proper noun, plural                           | Vikings             |\n",
    "| PDT     | Predeterminer                                 | both the boys       |\n",
    "| POS     | Possessive ending                             | friend’s            |\n",
    "| PP      | Personal pronoun                              | I, he, it           |\n",
    "| PPZ     | Possessive pronoun                            | my, his             |\n",
    "| RB      | Adverb                                        | however, usually    |\n",
    "| RBR     | Adverb, comparative                           | better              |\n",
    "| RBS     | Adverb, superlative                           | best                |\n",
    "| RP      | Particle                                      | give up             |\n",
    "| SENT    | Sentence-break punctuation                    | . ! ?               |\n",
    "| SYM     | Symbol                                        | / [ = *             |\n",
    "| TO      | Infinitive ‘to’                               | to go               |\n",
    "| UH      | Interjection                                  | uhhuhhuhh           |\n",
    "| VB      | Verb, base form                               | be                  |\n",
    "| VBD     | Verb, past tense                              | was, were           |\n",
    "| VBG     | Verb, gerund/present participle               | being               |\n",
    "| VBN     | Verb, past participle                         | been                |\n",
    "| VBP     | Verb, sing. present, non-3d                   | am, are             |\n",
    "| VBZ     | Verb, 3rd person sing. present                | is                  |\n",
    "| VH      | Verb have, base form                          | have                |\n",
    "| VHD     | Verb have, past tense                         | had                 |\n",
    "| VHG     | Verb have, gerund/present participle          | having              |\n",
    "| VHN     | Verb have, past participle                    | had                 |\n",
    "| VHP     | Verb have, sing. present, non-3d              | have                |\n",
    "| VHZ     | Verb have, 3rd person sing. present           | has                 |\n",
    "| VV      | Verb, base form                               | take                |\n",
    "| VVD     | Verb, past tense                              | took                |\n",
    "| VVG     | Verb, gerund/present participle               | taking              |\n",
    "| VVN     | Verb, past participle                         | taken               |\n",
    "| VVP     | Verb, sing. present, non-3d                   | take                |\n",
    "| VVZ     | Verb, 3rd person sing. present                | takes               |\n",
    "| WDT     | Wh-determiner                                 | which               |\n",
    "| WP      | Wh-pronoun                                    | who, what           |\n",
    "| WP$     | Possessive wh-pronoun                         | whose               |\n",
    "| WRB     | Wh-abverb                                     | where, when         |\n",
    "\n",
    "### Main Differences to the Default Penn Tagset\n",
    "- In TreeTagger:\n",
    "  - Distinguishes 'be' (VB) and 'have' (VH) from other (non-modal) verbs (VV).\n",
    "  - For proper nouns, NNP and NNPS have become NP and NPS.\n",
    "  - SENT for end-of-sentence punctuation (other punctuation tags may also differ).\n",
    "- In TreeTagger tool + Sketch Engine modifications:\n",
    "  - The word 'to' is tagged IN when used as a preposition and TO when used as an infinitive marker.\n",
    "\n",
    "### Bibliography\n",
    "\n",
    "M. Marcus, B. Santorini and M.A. Marcinkiewicz (1993). Building a large annotated corpus of English: The Penn Treebank. In Computational Linguistics, volume 19, number 2, pp. 313–330."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech POS Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumps over the lazy dog\n",
      "Dr Emily Wong PhD exclaimed OMG Despite reaching 78 billion in 2021 global population metrics like those in Section 123i3Aii remain baffling and then she added in a mix of French and English Cest incroyable no Check my blog at wwwemilywongsciencecom or email me at wongscienceworldcom for the full story on Mycobacterium tuberculosis complex MTBC research which is you know the stateoftheart stuffIve literally termed it the Occams razor of epidemiology\n"
     ]
    }
   ],
   "source": [
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "print(remove_punctuation(sentence))\n",
    "print(remove_punctuation(complex_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the quick brown fox jumps over the lazy dog. \n",
      " dr. emily wong, ph.d., exclaimed, 'omg! despite reaching 7.8 billion in 2021, global population metrics, like those in section 123(i)(3)(a)(ii), remain baffling,' and then she added in a mix of french and english, 'c’est incroyable, no? check my blog at www.emilywong-science.com or email me at wong@scienceworld.com for the full story on mycobacterium tuberculosis complex (mtbc) research, which is, you know, the state-of-the-art stuff—i've literally termed it \"the occam’s razor of epidemiology.\n"
     ]
    }
   ],
   "source": [
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "print(lowercase(sentence), \"\\n\", lowercase(complex_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t', 'h', 'e', ' ', 'q', 'u', 'i', 'c', 'k', ' ', 'b', 'r', 'o', 'w', 'n', ' ', 'f', 'o', 'x', ' ', 'j', 'u', 'm', 'p', 's', ' ', 'o', 'v', 'e', 'r', ' ', 't', 'h', 'e', ' ', 'l', 'a', 'z', 'y', ' ', 'd', 'o', 'g', '.'] \n",
      " ['d', 'r', '.', ' ', 'e', 'm', 'i', 'l', 'y', ' ', 'w', 'o', 'n', 'g', ',', ' ', 'p', 'h', '.', 'd', '.', ',', ' ', 'e', 'x', 'c', 'l', 'a', 'i', 'm', 'e', 'd', ',', ' ', \"'\", 'o', 'm', 'g', '!', ' ', 'd', 'e', 's', 'p', 'i', 't', 'e', ' ', 'r', 'e', 'a', 'c', 'h', 'i', 'n', 'g', ' ', '7', '.', '8', ' ', 'b', 'i', 'l', 'l', 'i', 'o', 'n', ' ', 'i', 'n', ' ', '2', '0', '2', '1', ',', ' ', 'g', 'l', 'o', 'b', 'a', 'l', ' ', 'p', 'o', 'p', 'u', 'l', 'a', 't', 'i', 'o', 'n', ' ', 'm', 'e', 't', 'r', 'i', 'c', 's', ',', ' ', 'l', 'i', 'k', 'e', ' ', 't', 'h', 'o', 's', 'e', ' ', 'i', 'n', ' ', 's', 'e', 'c', 't', 'i', 'o', 'n', ' ', '1', '2', '3', '(', 'i', ')', '(', '3', ')', '(', 'a', ')', '(', 'i', 'i', ')', ',', ' ', 'r', 'e', 'm', 'a', 'i', 'n', ' ', 'b', 'a', 'f', 'f', 'l', 'i', 'n', 'g', ',', \"'\", ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', 'n', ' ', 's', 'h', 'e', ' ', 'a', 'd', 'd', 'e', 'd', ' ', 'i', 'n', ' ', 'a', ' ', 'm', 'i', 'x', ' ', 'o', 'f', ' ', 'f', 'r', 'e', 'n', 'c', 'h', ' ', 'a', 'n', 'd', ' ', 'e', 'n', 'g', 'l', 'i', 's', 'h', ',', ' ', \"'\", 'c', '’', 'e', 's', 't', ' ', 'i', 'n', 'c', 'r', 'o', 'y', 'a', 'b', 'l', 'e', ',', ' ', 'n', 'o', '?', ' ', 'c', 'h', 'e', 'c', 'k', ' ', 'm', 'y', ' ', 'b', 'l', 'o', 'g', ' ', 'a', 't', ' ', 'w', 'w', 'w', '.', 'e', 'm', 'i', 'l', 'y', 'w', 'o', 'n', 'g', '-', 's', 'c', 'i', 'e', 'n', 'c', 'e', '.', 'c', 'o', 'm', ' ', 'o', 'r', ' ', 'e', 'm', 'a', 'i', 'l', ' ', 'm', 'e', ' ', 'a', 't', ' ', 'w', 'o', 'n', 'g', '@', 's', 'c', 'i', 'e', 'n', 'c', 'e', 'w', 'o', 'r', 'l', 'd', '.', 'c', 'o', 'm', ' ', 'f', 'o', 'r', ' ', 't', 'h', 'e', ' ', 'f', 'u', 'l', 'l', ' ', 's', 't', 'o', 'r', 'y', ' ', 'o', 'n', ' ', 'm', 'y', 'c', 'o', 'b', 'a', 'c', 't', 'e', 'r', 'i', 'u', 'm', ' ', 't', 'u', 'b', 'e', 'r', 'c', 'u', 'l', 'o', 's', 'i', 's', ' ', 'c', 'o', 'm', 'p', 'l', 'e', 'x', ' ', '(', 'm', 't', 'b', 'c', ')', ' ', 'r', 'e', 's', 'e', 'a', 'r', 'c', 'h', ',', ' ', 'w', 'h', 'i', 'c', 'h', ' ', 'i', 's', ',', ' ', 'y', 'o', 'u', ' ', 'k', 'n', 'o', 'w', ',', ' ', 't', 'h', 'e', ' ', 's', 't', 'a', 't', 'e', '-', 'o', 'f', '-', 't', 'h', 'e', '-', 'a', 'r', 't', ' ', 's', 't', 'u', 'f', 'f', '—', 'i', \"'\", 'v', 'e', ' ', 'l', 'i', 't', 'e', 'r', 'a', 'l', 'l', 'y', ' ', 't', 'e', 'r', 'm', 'e', 'd', ' ', 'i', 't', ' ', '\"', 't', 'h', 'e', ' ', 'o', 'c', 'c', 'a', 'm', '’', 's', ' ', 'r', 'a', 'z', 'o', 'r', ' ', 'o', 'f', ' ', 'e', 'p', 'i', 'd', 'e', 'm', 'i', 'o', 'l', 'o', 'g', 'y', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "def stemming(words):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(stemming(sentence), \"\\n\", stemming(complex_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ganad\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'h', 'e', ' ', 'q', 'u', 'i', 'c', 'k', ' ', 'b', 'r', 'o', 'w', 'n', ' ', 'f', 'o', 'x', ' ', 'j', 'u', 'm', 'p', 's', ' ', 'o', 'v', 'e', 'r', ' ', 't', 'h', 'e', ' ', 'l', 'a', 'z', 'y', ' ', 'd', 'o', 'g', '.'] \n",
      " ['D', 'r', '.', ' ', 'E', 'm', 'i', 'l', 'y', ' ', 'W', 'o', 'n', 'g', ',', ' ', 'P', 'h', '.', 'D', '.', ',', ' ', 'e', 'x', 'c', 'l', 'a', 'i', 'm', 'e', 'd', ',', ' ', \"'\", 'O', 'M', 'G', '!', ' ', 'D', 'e', 's', 'p', 'i', 't', 'e', ' ', 'r', 'e', 'a', 'c', 'h', 'i', 'n', 'g', ' ', '7', '.', '8', ' ', 'b', 'i', 'l', 'l', 'i', 'o', 'n', ' ', 'i', 'n', ' ', '2', '0', '2', '1', ',', ' ', 'g', 'l', 'o', 'b', 'a', 'l', ' ', 'p', 'o', 'p', 'u', 'l', 'a', 't', 'i', 'o', 'n', ' ', 'm', 'e', 't', 'r', 'i', 'c', 's', ',', ' ', 'l', 'i', 'k', 'e', ' ', 't', 'h', 'o', 's', 'e', ' ', 'i', 'n', ' ', 'S', 'e', 'c', 't', 'i', 'o', 'n', ' ', '1', '2', '3', '(', 'i', ')', '(', '3', ')', '(', 'A', ')', '(', 'i', 'i', ')', ',', ' ', 'r', 'e', 'm', 'a', 'i', 'n', ' ', 'b', 'a', 'f', 'f', 'l', 'i', 'n', 'g', ',', \"'\", ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', 'n', ' ', 's', 'h', 'e', ' ', 'a', 'd', 'd', 'e', 'd', ' ', 'i', 'n', ' ', 'a', ' ', 'm', 'i', 'x', ' ', 'o', 'f', ' ', 'F', 'r', 'e', 'n', 'c', 'h', ' ', 'a', 'n', 'd', ' ', 'E', 'n', 'g', 'l', 'i', 's', 'h', ',', ' ', \"'\", 'C', '’', 'e', 's', 't', ' ', 'i', 'n', 'c', 'r', 'o', 'y', 'a', 'b', 'l', 'e', ',', ' ', 'n', 'o', '?', ' ', 'C', 'h', 'e', 'c', 'k', ' ', 'm', 'y', ' ', 'b', 'l', 'o', 'g', ' ', 'a', 't', ' ', 'w', 'w', 'w', '.', 'e', 'm', 'i', 'l', 'y', 'w', 'o', 'n', 'g', '-', 's', 'c', 'i', 'e', 'n', 'c', 'e', '.', 'c', 'o', 'm', ' ', 'o', 'r', ' ', 'e', 'm', 'a', 'i', 'l', ' ', 'm', 'e', ' ', 'a', 't', ' ', 'w', 'o', 'n', 'g', '@', 's', 'c', 'i', 'e', 'n', 'c', 'e', 'w', 'o', 'r', 'l', 'd', '.', 'c', 'o', 'm', ' ', 'f', 'o', 'r', ' ', 't', 'h', 'e', ' ', 'f', 'u', 'l', 'l', ' ', 's', 't', 'o', 'r', 'y', ' ', 'o', 'n', ' ', 'M', 'y', 'c', 'o', 'b', 'a', 'c', 't', 'e', 'r', 'i', 'u', 'm', ' ', 't', 'u', 'b', 'e', 'r', 'c', 'u', 'l', 'o', 's', 'i', 's', ' ', 'c', 'o', 'm', 'p', 'l', 'e', 'x', ' ', '(', 'M', 'T', 'B', 'C', ')', ' ', 'r', 'e', 's', 'e', 'a', 'r', 'c', 'h', ',', ' ', 'w', 'h', 'i', 'c', 'h', ' ', 'i', 's', ',', ' ', 'y', 'o', 'u', ' ', 'k', 'n', 'o', 'w', ',', ' ', 't', 'h', 'e', ' ', 's', 't', 'a', 't', 'e', '-', 'o', 'f', '-', 't', 'h', 'e', '-', 'a', 'r', 't', ' ', 's', 't', 'u', 'f', 'f', '—', 'I', \"'\", 'v', 'e', ' ', 'l', 'i', 't', 'e', 'r', 'a', 'l', 'l', 'y', ' ', 't', 'e', 'r', 'm', 'e', 'd', ' ', 'i', 't', ' ', '\"', 't', 'h', 'e', ' ', 'O', 'c', 'c', 'a', 'm', '’', 's', ' ', 'r', 'a', 'z', 'o', 'r', ' ', 'o', 'f', ' ', 'e', 'p', 'i', 'd', 'e', 'm', 'i', 'o', 'l', 'o', 'g', 'y', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def lemmatization(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "print(lemmatization(sentence), \"\\n\", lemmatization(complex_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pickle\n",
    "\n",
    "model_dir = r'C:\\Users\\ganad\\Desktop\\Resume Work\\NLP_HUB\\models'\n",
    "\n",
    "# Fetch the dataset\n",
    "data = fetch_20newsgroups(subset='train')\n",
    "categories = data.target_names\n",
    "\n",
    "# Create pipelines for different classifiers\n",
    "models = {\n",
    "    'Naive Bayes': make_pipeline(TfidfVectorizer(), MultinomialNB()),\n",
    "    'SVM': make_pipeline(TfidfVectorizer(), SVC(probability=True)),\n",
    "    'Logistic Regression': make_pipeline(TfidfVectorizer(), LogisticRegression())\n",
    "}\n",
    "\n",
    "# Train and save each model in the specified directory\n",
    "for name, model in models.items():\n",
    "    model.fit(data.data, data.target)\n",
    "    model_path = os.path.join(model_dir, f'{name.lower().replace(\" \", \"_\")}_classifier.pkl')\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction from Naive Bayes: rec.motorcycles\n",
      "Prediction from SVM: rec.motorcycles\n",
      "Prediction from Logistic Regression: rec.motorcycles\n",
      "Prediction from Naive Bayes: soc.religion.christian\n",
      "Prediction from SVM: sci.med\n",
      "Prediction from Logistic Regression: sci.med\n"
     ]
    }
   ],
   "source": [
    "# Define the directory where models are saved\n",
    "model_dir = r'C:\\Users\\ganad\\Desktop\\Resume Work\\NLP_HUB\\models'\n",
    "\n",
    "# Dictionary to map model names to file names\n",
    "model_files = {\n",
    "    'Naive Bayes': os.path.join(model_dir, 'naive_bayes_classifier.pkl'),\n",
    "    'SVM': os.path.join(model_dir, 'svm_classifier.pkl'),\n",
    "    'Logistic Regression': os.path.join(model_dir, 'logistic_regression_classifier.pkl')\n",
    "}\n",
    "\n",
    "# Function to load a model\n",
    "def load_model(model_name):\n",
    "    with open(model_files[model_name], 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    return model\n",
    "\n",
    "# Load the dataset to fetch categories\n",
    "data = fetch_20newsgroups(subset='train')\n",
    "categories = data.target_names\n",
    "\n",
    "# Load all models\n",
    "models = {name: load_model(name) for name in model_files.keys()}\n",
    "\n",
    "\n",
    "# Print predictions from each model\n",
    "for name, model in models.items():\n",
    "    prediction = model.predict([sentence])\n",
    "    print(f\"Prediction from {name}: {categories[prediction[0]]}\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    prediction = model.predict([complex_sentence])\n",
    "    print(f\"Prediction from {name}: {categories[prediction[0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "- Bird, Steven, Edward Loper and Ewan Klein (2009).\n",
    "Natural Language Processing with Python.  O'Reilly Media Inc.\n",
    "- https://neptune.ai/blog/tokenization-in-nlp\n",
    "- https://www.sketchengine.eu/penn-treebank-tagset/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
