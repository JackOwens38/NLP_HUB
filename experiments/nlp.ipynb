{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Concepts using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ganad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ganad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "from nltk.tokenize import (word_tokenize,\n",
    "                           sent_tokenize,\n",
    "                           TreebankWordTokenizer,\n",
    "                           wordpunct_tokenize,\n",
    "                           TweetTokenizer,\n",
    "                           MWETokenizer)\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a more complex sentence to showcase tokenization\n",
    "complex_sentence = \"\"\"Dr. Emily Wong, Ph.D., exclaimed, 'OMG! Despite reaching 7.8 billion in 2021, global population metrics, like those in Section 123(i)(3)(A)(ii), remain baffling,' and then she added in a mix of French and English, 'C’est incroyable, no? Check my blog at www.emilywong-science.com or email me at wong@scienceworld.com for the full story on Mycobacterium tuberculosis complex (MTBC) research, which is, you know, the state-of-the-art stuff—I've literally termed it \"the Occam’s razor of epidemiology.\"\"\"\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['The quick brown fox jumps over the lazy dog.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokens_word = nltk.word_tokenize(sentence)\n",
    "token_sent = nltk.sent_tokenize(sentence)\n",
    "token_treebank = TreebankWordTokenizer().tokenize(sentence)\n",
    "token_wordpunct = wordpunct_tokenize(sentence)\n",
    "token_tweet = TweetTokenizer().tokenize(sentence)\n",
    "token_mwe = MWETokenizer().tokenize(sentence.split())\n",
    "\n",
    "\n",
    "display(tokens_word,\n",
    "        token_sent,\n",
    "        token_treebank,\n",
    "        token_wordpunct,\n",
    "        token_tweet,\n",
    "        token_mwe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dr.', 'Emily', 'Wong', ',', 'Ph.D.', ',', 'exclaimed', ',', \"'OMG\", '!', 'Despite', 'reaching', '7.8', 'billion', 'in', '2021', ',', 'global', 'population', 'metrics', ',', 'like', 'those', 'in', 'Section', '123', '(', 'i', ')', '(', '3', ')', '(', 'A', ')', '(', 'ii', ')', ',', 'remain', 'baffling', ',', \"'\", 'and', 'then', 'she', 'added', 'in', 'a', 'mix', 'of', 'French', 'and', 'English', ',', \"'\", 'C', '’', 'est', 'incroyable', ',', 'no', '?', 'Check', 'my', 'blog', 'at', 'www.emilywong-science.com', 'or', 'email', 'me', 'at', 'wong', '@', 'scienceworld.com', 'for', 'the', 'full', 'story', 'on', 'Mycobacterium', 'tuberculosis', 'complex', '(', 'MTBC', ')', 'research', ',', 'which', 'is', ',', 'you', 'know', ',', 'the', 'state-of-the-art', 'stuff—I', \"'ve\", 'literally', 'termed', 'it', '``', 'the', 'Occam', '’', 's', 'razor', 'of', 'epidemiology', '.']\n",
      "\n",
      "[\"Dr. Emily Wong, Ph.D., exclaimed, 'OMG!\", \"Despite reaching 7.8 billion in 2021, global population metrics, like those in Section 123(i)(3)(A)(ii), remain baffling,' and then she added in a mix of French and English, 'C’est incroyable, no?\", 'Check my blog at www.emilywong-science.com or email me at wong@scienceworld.com for the full story on Mycobacterium tuberculosis complex (MTBC) research, which is, you know, the state-of-the-art stuff—I\\'ve literally termed it \"the Occam’s razor of epidemiology.']\n",
      "\n",
      "['Dr.', 'Emily', 'Wong', ',', 'Ph.D.', ',', 'exclaimed', ',', \"'OMG\", '!', 'Despite', 'reaching', '7.8', 'billion', 'in', '2021', ',', 'global', 'population', 'metrics', ',', 'like', 'those', 'in', 'Section', '123', '(', 'i', ')', '(', '3', ')', '(', 'A', ')', '(', 'ii', ')', ',', 'remain', 'baffling', ',', \"'\", 'and', 'then', 'she', 'added', 'in', 'a', 'mix', 'of', 'French', 'and', 'English', ',', \"'C’est\", 'incroyable', ',', 'no', '?', 'Check', 'my', 'blog', 'at', 'www.emilywong-science.com', 'or', 'email', 'me', 'at', 'wong', '@', 'scienceworld.com', 'for', 'the', 'full', 'story', 'on', 'Mycobacterium', 'tuberculosis', 'complex', '(', 'MTBC', ')', 'research', ',', 'which', 'is', ',', 'you', 'know', ',', 'the', 'state-of-the-art', 'stuff—I', \"'ve\", 'literally', 'termed', 'it', '``', 'the', 'Occam’s', 'razor', 'of', 'epidemiology', '.']\n",
      "\n",
      "['Dr', '.', 'Emily', 'Wong', ',', 'Ph', '.', 'D', '.,', 'exclaimed', ',', \"'\", 'OMG', '!', 'Despite', 'reaching', '7', '.', '8', 'billion', 'in', '2021', ',', 'global', 'population', 'metrics', ',', 'like', 'those', 'in', 'Section', '123', '(', 'i', ')(', '3', ')(', 'A', ')(', 'ii', '),', 'remain', 'baffling', \",'\", 'and', 'then', 'she', 'added', 'in', 'a', 'mix', 'of', 'French', 'and', 'English', ',', \"'\", 'C', '’', 'est', 'incroyable', ',', 'no', '?', 'Check', 'my', 'blog', 'at', 'www', '.', 'emilywong', '-', 'science', '.', 'com', 'or', 'email', 'me', 'at', 'wong', '@', 'scienceworld', '.', 'com', 'for', 'the', 'full', 'story', 'on', 'Mycobacterium', 'tuberculosis', 'complex', '(', 'MTBC', ')', 'research', ',', 'which', 'is', ',', 'you', 'know', ',', 'the', 'state', '-', 'of', '-', 'the', '-', 'art', 'stuff', '—', 'I', \"'\", 've', 'literally', 'termed', 'it', '\"', 'the', 'Occam', '’', 's', 'razor', 'of', 'epidemiology', '.']\n",
      "\n",
      "['Dr', '.', 'Emily', 'Wong', ',', 'Ph', '.', 'D', '.', ',', 'exclaimed', ',', \"'\", 'OMG', '!', 'Despite', 'reaching', '7.8', 'billion', 'in', '2021', ',', 'global', 'population', 'metrics', ',', 'like', 'those', 'in', 'Section', '123', '(', 'i', ')', '(', '3', ')', '(', 'A', ')', '(', 'ii', ')', ',', 'remain', 'baffling', ',', \"'\", 'and', 'then', 'she', 'added', 'in', 'a', 'mix', 'of', 'French', 'and', 'English', ',', \"'\", 'C', '’', 'est', 'incroyable', ',', 'no', '?', 'Check', 'my', 'blog', 'at', 'www.emilywong-science.com', 'or', 'email', 'me', 'at', 'wong@scienceworld.com', 'for', 'the', 'full', 'story', 'on', 'Mycobacterium', 'tuberculosis', 'complex', '(', 'MTBC', ')', 'research', ',', 'which', 'is', ',', 'you', 'know', ',', 'the', 'state-of-the-art', 'stuff', '—', \"I've\", 'literally', 'termed', 'it', '\"', 'the', 'Occam', '’', 's', 'razor', 'of', 'epidemiology', '.']\n",
      "\n",
      "['Dr.', 'Emily', 'Wong,', 'Ph.D.,', 'exclaimed,', \"'OMG!\", 'Despite', 'reaching', '7.8', 'billion', 'in', '2021,', 'global', 'population', 'metrics,', 'like', 'those', 'in', 'Section', '123(i)(3)(A)(ii),', 'remain', \"baffling,'\", 'and', 'then', 'she', 'added', 'in', 'a', 'mix', 'of', 'French', 'and', 'English,', \"'C’est\", 'incroyable,', 'no?', 'Check', 'my', 'blog', 'at', 'www.emilywong-science.com', 'or', 'email', 'me', 'at', 'wong@scienceworld.com', 'for', 'the', 'full', 'story', 'on', 'Mycobacterium', 'tuberculosis', 'complex', '(MTBC)', 'research,', 'which', 'is,', 'you', 'know,', 'the', 'state-of-the-art', \"stuff—I've\", 'literally', 'termed', 'it', '\"the', 'Occam’s', 'razor', 'of', 'epidemiology.']\n"
     ]
    }
   ],
   "source": [
    "tokens_word_c = nltk.word_tokenize(complex_sentence)\n",
    "token_sent_c = nltk.sent_tokenize(complex_sentence)\n",
    "token_treebank_c = TreebankWordTokenizer().tokenize(complex_sentence)\n",
    "token_wordpunct_c = wordpunct_tokenize(complex_sentence)\n",
    "token_tweet_c = TweetTokenizer().tokenize(complex_sentence)\n",
    "token_mwe_c = MWETokenizer().tokenize(complex_sentence.split())\n",
    "\n",
    "print(tokens_word_c)\n",
    "print()\n",
    "print(token_sent_c)\n",
    "print()\n",
    "print(token_treebank_c)\n",
    "print()\n",
    "print(token_wordpunct_c)\n",
    "print()\n",
    "print(token_tweet_c)\n",
    "print()\n",
    "print(token_mwe_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "- Word Tokenizer and the Treebank Tokenizer seems pretty good compared to the others\n",
    "\n",
    "- Picking the right tokenizer depends on the task at hand. For example, if you are working on a sentiment analysis task, you might want to use a tokenizer that preserves emoticons and hashtags. If you are working on a machine translation task, you might want to use a tokenizer that preserves punctuation and capitalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penn Treebank Tagset Overview\n",
    "\n",
    "A tagset is a list of part-of-speech tags, i.e., labels used to indicate the part of speech and often also other grammatical categories (case, tense etc.) of each token in a text corpus.\n",
    "\n",
    "#### Introduction to Penn Treebank Tagset\n",
    "\n",
    "The English Penn Treebank tagset is utilized with English corpora annotated by the TreeTagger tool, developed by Helmut Schmid in the TC project at the Institute for Computational Linguistics of the University of Stuttgart. This version of the tagset contains modifications developed by Sketch Engine (earlier version).\n",
    "\n",
    "[See a more recent version of this tagset](https://www.sketchengine.eu/penn-treebank-tagset/).\n",
    "\n",
    "### What is a POS Tag?\n",
    "\n",
    "POS tags classify words into grammatical categories which can help in understanding the structure and context of text. The table below shows the English Penn TreeBank tagset with Sketch Engine modifications (earlier version).\n",
    "\n",
    "Example: Using `[tag=\"NNS\"]` finds all nouns in the plural, e.g., people, years when used in the CQL concordance search (always use straight double quotation marks in CQL).\n",
    "\n",
    "| POS Tag | Description                                   | Example             |\n",
    "|---------|-----------------------------------------------|---------------------|\n",
    "| CC      | Coordinating conjunction                      | and                 |\n",
    "| CD      | Cardinal number                               | 1, third            |\n",
    "| DT      | Determiner                                    | the                 |\n",
    "| EX      | Existential there                             | there is            |\n",
    "| FW      | Foreign word                                  | les                 |\n",
    "| IN      | Preposition, subordinating conjunction        | in, of, like        |\n",
    "| IN/that | That as subordinator                          | that                |\n",
    "| JJ      | Adjective                                     | green               |\n",
    "| JJR     | Adjective, comparative                        | greener             |\n",
    "| JJS     | Adjective, superlative                        | greenest            |\n",
    "| LS      | List marker                                   | 1)                  |\n",
    "| MD      | Modal                                         | could, will         |\n",
    "| NN      | Noun, singular or mass                        | table               |\n",
    "| NNS     | Noun, plural                                  | tables              |\n",
    "| NP      | Proper noun, singular                         | John                |\n",
    "| NPS     | Proper noun, plural                           | Vikings             |\n",
    "| PDT     | Predeterminer                                 | both the boys       |\n",
    "| POS     | Possessive ending                             | friend’s            |\n",
    "| PP      | Personal pronoun                              | I, he, it           |\n",
    "| PPZ     | Possessive pronoun                            | my, his             |\n",
    "| RB      | Adverb                                        | however, usually    |\n",
    "| RBR     | Adverb, comparative                           | better              |\n",
    "| RBS     | Adverb, superlative                           | best                |\n",
    "| RP      | Particle                                      | give up             |\n",
    "| SENT    | Sentence-break punctuation                    | . ! ?               |\n",
    "| SYM     | Symbol                                        | / [ = *             |\n",
    "| TO      | Infinitive ‘to’                               | to go               |\n",
    "| UH      | Interjection                                  | uhhuhhuhh           |\n",
    "| VB      | Verb, base form                               | be                  |\n",
    "| VBD     | Verb, past tense                              | was, were           |\n",
    "| VBG     | Verb, gerund/present participle               | being               |\n",
    "| VBN     | Verb, past participle                         | been                |\n",
    "| VBP     | Verb, sing. present, non-3d                   | am, are             |\n",
    "| VBZ     | Verb, 3rd person sing. present                | is                  |\n",
    "| VH      | Verb have, base form                          | have                |\n",
    "| VHD     | Verb have, past tense                         | had                 |\n",
    "| VHG     | Verb have, gerund/present participle          | having              |\n",
    "| VHN     | Verb have, past participle                    | had                 |\n",
    "| VHP     | Verb have, sing. present, non-3d              | have                |\n",
    "| VHZ     | Verb have, 3rd person sing. present           | has                 |\n",
    "| VV      | Verb, base form                               | take                |\n",
    "| VVD     | Verb, past tense                              | took                |\n",
    "| VVG     | Verb, gerund/present participle               | taking              |\n",
    "| VVN     | Verb, past participle                         | taken               |\n",
    "| VVP     | Verb, sing. present, non-3d                   | take                |\n",
    "| VVZ     | Verb, 3rd person sing. present                | takes               |\n",
    "| WDT     | Wh-determiner                                 | which               |\n",
    "| WP      | Wh-pronoun                                    | who, what           |\n",
    "| WP$     | Possessive wh-pronoun                         | whose               |\n",
    "| WRB     | Wh-abverb                                     | where, when         |\n",
    "\n",
    "### Main Differences to the Default Penn Tagset\n",
    "- In TreeTagger:\n",
    "  - Distinguishes 'be' (VB) and 'have' (VH) from other (non-modal) verbs (VV).\n",
    "  - For proper nouns, NNP and NNPS have become NP and NPS.\n",
    "  - SENT for end-of-sentence punctuation (other punctuation tags may also differ).\n",
    "- In TreeTagger tool + Sketch Engine modifications:\n",
    "  - The word 'to' is tagged IN when used as a preposition and TO when used as an infinitive marker.\n",
    "\n",
    "### Bibliography\n",
    "\n",
    "M. Marcus, B. Santorini and M.A. Marcinkiewicz (1993). Building a large annotated corpus of English: The Penn Treebank. In Computational Linguistics, volume 19, number 2, pp. 313–330."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech POS Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumps over the lazy dog\n",
      "Dr Emily Wong PhD exclaimed OMG Despite reaching 78 billion in 2021 global population metrics like those in Section 123i3Aii remain baffling and then she added in a mix of French and English Cest incroyable no Check my blog at wwwemilywongsciencecom or email me at wongscienceworldcom for the full story on Mycobacterium tuberculosis complex MTBC research which is you know the stateoftheart stuffIve literally termed it the Occams razor of epidemiology\n"
     ]
    }
   ],
   "source": [
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "print(remove_punctuation(sentence))\n",
    "print(remove_punctuation(complex_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the quick brown fox jumps over the lazy dog. \n",
      " dr. emily wong, ph.d., exclaimed, 'omg! despite reaching 7.8 billion in 2021, global population metrics, like those in section 123(i)(3)(a)(ii), remain baffling,' and then she added in a mix of french and english, 'c’est incroyable, no? check my blog at www.emilywong-science.com or email me at wong@scienceworld.com for the full story on mycobacterium tuberculosis complex (mtbc) research, which is, you know, the state-of-the-art stuff—i've literally termed it \"the occam’s razor of epidemiology.\n"
     ]
    }
   ],
   "source": [
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "print(lowercase(sentence), \"\\n\", lowercase(complex_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t', 'h', 'e', ' ', 'q', 'u', 'i', 'c', 'k', ' ', 'b', 'r', 'o', 'w', 'n', ' ', 'f', 'o', 'x', ' ', 'j', 'u', 'm', 'p', 's', ' ', 'o', 'v', 'e', 'r', ' ', 't', 'h', 'e', ' ', 'l', 'a', 'z', 'y', ' ', 'd', 'o', 'g', '.'] \n",
      " ['d', 'r', '.', ' ', 'e', 'm', 'i', 'l', 'y', ' ', 'w', 'o', 'n', 'g', ',', ' ', 'p', 'h', '.', 'd', '.', ',', ' ', 'e', 'x', 'c', 'l', 'a', 'i', 'm', 'e', 'd', ',', ' ', \"'\", 'o', 'm', 'g', '!', ' ', 'd', 'e', 's', 'p', 'i', 't', 'e', ' ', 'r', 'e', 'a', 'c', 'h', 'i', 'n', 'g', ' ', '7', '.', '8', ' ', 'b', 'i', 'l', 'l', 'i', 'o', 'n', ' ', 'i', 'n', ' ', '2', '0', '2', '1', ',', ' ', 'g', 'l', 'o', 'b', 'a', 'l', ' ', 'p', 'o', 'p', 'u', 'l', 'a', 't', 'i', 'o', 'n', ' ', 'm', 'e', 't', 'r', 'i', 'c', 's', ',', ' ', 'l', 'i', 'k', 'e', ' ', 't', 'h', 'o', 's', 'e', ' ', 'i', 'n', ' ', 's', 'e', 'c', 't', 'i', 'o', 'n', ' ', '1', '2', '3', '(', 'i', ')', '(', '3', ')', '(', 'a', ')', '(', 'i', 'i', ')', ',', ' ', 'r', 'e', 'm', 'a', 'i', 'n', ' ', 'b', 'a', 'f', 'f', 'l', 'i', 'n', 'g', ',', \"'\", ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', 'n', ' ', 's', 'h', 'e', ' ', 'a', 'd', 'd', 'e', 'd', ' ', 'i', 'n', ' ', 'a', ' ', 'm', 'i', 'x', ' ', 'o', 'f', ' ', 'f', 'r', 'e', 'n', 'c', 'h', ' ', 'a', 'n', 'd', ' ', 'e', 'n', 'g', 'l', 'i', 's', 'h', ',', ' ', \"'\", 'c', '’', 'e', 's', 't', ' ', 'i', 'n', 'c', 'r', 'o', 'y', 'a', 'b', 'l', 'e', ',', ' ', 'n', 'o', '?', ' ', 'c', 'h', 'e', 'c', 'k', ' ', 'm', 'y', ' ', 'b', 'l', 'o', 'g', ' ', 'a', 't', ' ', 'w', 'w', 'w', '.', 'e', 'm', 'i', 'l', 'y', 'w', 'o', 'n', 'g', '-', 's', 'c', 'i', 'e', 'n', 'c', 'e', '.', 'c', 'o', 'm', ' ', 'o', 'r', ' ', 'e', 'm', 'a', 'i', 'l', ' ', 'm', 'e', ' ', 'a', 't', ' ', 'w', 'o', 'n', 'g', '@', 's', 'c', 'i', 'e', 'n', 'c', 'e', 'w', 'o', 'r', 'l', 'd', '.', 'c', 'o', 'm', ' ', 'f', 'o', 'r', ' ', 't', 'h', 'e', ' ', 'f', 'u', 'l', 'l', ' ', 's', 't', 'o', 'r', 'y', ' ', 'o', 'n', ' ', 'm', 'y', 'c', 'o', 'b', 'a', 'c', 't', 'e', 'r', 'i', 'u', 'm', ' ', 't', 'u', 'b', 'e', 'r', 'c', 'u', 'l', 'o', 's', 'i', 's', ' ', 'c', 'o', 'm', 'p', 'l', 'e', 'x', ' ', '(', 'm', 't', 'b', 'c', ')', ' ', 'r', 'e', 's', 'e', 'a', 'r', 'c', 'h', ',', ' ', 'w', 'h', 'i', 'c', 'h', ' ', 'i', 's', ',', ' ', 'y', 'o', 'u', ' ', 'k', 'n', 'o', 'w', ',', ' ', 't', 'h', 'e', ' ', 's', 't', 'a', 't', 'e', '-', 'o', 'f', '-', 't', 'h', 'e', '-', 'a', 'r', 't', ' ', 's', 't', 'u', 'f', 'f', '—', 'i', \"'\", 'v', 'e', ' ', 'l', 'i', 't', 'e', 'r', 'a', 'l', 'l', 'y', ' ', 't', 'e', 'r', 'm', 'e', 'd', ' ', 'i', 't', ' ', '\"', 't', 'h', 'e', ' ', 'o', 'c', 'c', 'a', 'm', '’', 's', ' ', 'r', 'a', 'z', 'o', 'r', ' ', 'o', 'f', ' ', 'e', 'p', 'i', 'd', 'e', 'm', 'i', 'o', 'l', 'o', 'g', 'y', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "def stemming(words):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(stemming(sentence), \"\\n\", stemming(complex_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ganad\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'h', 'e', ' ', 'q', 'u', 'i', 'c', 'k', ' ', 'b', 'r', 'o', 'w', 'n', ' ', 'f', 'o', 'x', ' ', 'j', 'u', 'm', 'p', 's', ' ', 'o', 'v', 'e', 'r', ' ', 't', 'h', 'e', ' ', 'l', 'a', 'z', 'y', ' ', 'd', 'o', 'g', '.'] \n",
      " ['D', 'r', '.', ' ', 'E', 'm', 'i', 'l', 'y', ' ', 'W', 'o', 'n', 'g', ',', ' ', 'P', 'h', '.', 'D', '.', ',', ' ', 'e', 'x', 'c', 'l', 'a', 'i', 'm', 'e', 'd', ',', ' ', \"'\", 'O', 'M', 'G', '!', ' ', 'D', 'e', 's', 'p', 'i', 't', 'e', ' ', 'r', 'e', 'a', 'c', 'h', 'i', 'n', 'g', ' ', '7', '.', '8', ' ', 'b', 'i', 'l', 'l', 'i', 'o', 'n', ' ', 'i', 'n', ' ', '2', '0', '2', '1', ',', ' ', 'g', 'l', 'o', 'b', 'a', 'l', ' ', 'p', 'o', 'p', 'u', 'l', 'a', 't', 'i', 'o', 'n', ' ', 'm', 'e', 't', 'r', 'i', 'c', 's', ',', ' ', 'l', 'i', 'k', 'e', ' ', 't', 'h', 'o', 's', 'e', ' ', 'i', 'n', ' ', 'S', 'e', 'c', 't', 'i', 'o', 'n', ' ', '1', '2', '3', '(', 'i', ')', '(', '3', ')', '(', 'A', ')', '(', 'i', 'i', ')', ',', ' ', 'r', 'e', 'm', 'a', 'i', 'n', ' ', 'b', 'a', 'f', 'f', 'l', 'i', 'n', 'g', ',', \"'\", ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', 'n', ' ', 's', 'h', 'e', ' ', 'a', 'd', 'd', 'e', 'd', ' ', 'i', 'n', ' ', 'a', ' ', 'm', 'i', 'x', ' ', 'o', 'f', ' ', 'F', 'r', 'e', 'n', 'c', 'h', ' ', 'a', 'n', 'd', ' ', 'E', 'n', 'g', 'l', 'i', 's', 'h', ',', ' ', \"'\", 'C', '’', 'e', 's', 't', ' ', 'i', 'n', 'c', 'r', 'o', 'y', 'a', 'b', 'l', 'e', ',', ' ', 'n', 'o', '?', ' ', 'C', 'h', 'e', 'c', 'k', ' ', 'm', 'y', ' ', 'b', 'l', 'o', 'g', ' ', 'a', 't', ' ', 'w', 'w', 'w', '.', 'e', 'm', 'i', 'l', 'y', 'w', 'o', 'n', 'g', '-', 's', 'c', 'i', 'e', 'n', 'c', 'e', '.', 'c', 'o', 'm', ' ', 'o', 'r', ' ', 'e', 'm', 'a', 'i', 'l', ' ', 'm', 'e', ' ', 'a', 't', ' ', 'w', 'o', 'n', 'g', '@', 's', 'c', 'i', 'e', 'n', 'c', 'e', 'w', 'o', 'r', 'l', 'd', '.', 'c', 'o', 'm', ' ', 'f', 'o', 'r', ' ', 't', 'h', 'e', ' ', 'f', 'u', 'l', 'l', ' ', 's', 't', 'o', 'r', 'y', ' ', 'o', 'n', ' ', 'M', 'y', 'c', 'o', 'b', 'a', 'c', 't', 'e', 'r', 'i', 'u', 'm', ' ', 't', 'u', 'b', 'e', 'r', 'c', 'u', 'l', 'o', 's', 'i', 's', ' ', 'c', 'o', 'm', 'p', 'l', 'e', 'x', ' ', '(', 'M', 'T', 'B', 'C', ')', ' ', 'r', 'e', 's', 'e', 'a', 'r', 'c', 'h', ',', ' ', 'w', 'h', 'i', 'c', 'h', ' ', 'i', 's', ',', ' ', 'y', 'o', 'u', ' ', 'k', 'n', 'o', 'w', ',', ' ', 't', 'h', 'e', ' ', 's', 't', 'a', 't', 'e', '-', 'o', 'f', '-', 't', 'h', 'e', '-', 'a', 'r', 't', ' ', 's', 't', 'u', 'f', 'f', '—', 'I', \"'\", 'v', 'e', ' ', 'l', 'i', 't', 'e', 'r', 'a', 'l', 'l', 'y', ' ', 't', 'e', 'r', 'm', 'e', 'd', ' ', 'i', 't', ' ', '\"', 't', 'h', 'e', ' ', 'O', 'c', 'c', 'a', 'm', '’', 's', ' ', 'r', 'a', 'z', 'o', 'r', ' ', 'o', 'f', ' ', 'e', 'p', 'i', 'd', 'e', 'm', 'i', 'o', 'l', 'o', 'g', 'y', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def lemmatization(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "print(lemmatization(sentence), \"\\n\", lemmatization(complex_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pickle\n",
    "\n",
    "model_dir = r'C:\\Users\\ganad\\Desktop\\Resume Work\\NLP_HUB\\models'\n",
    "\n",
    "# Fetch the dataset\n",
    "data = fetch_20newsgroups(subset='train')\n",
    "categories = data.target_names\n",
    "\n",
    "# Create pipelines for different classifiers\n",
    "models = {\n",
    "    'Naive Bayes': make_pipeline(TfidfVectorizer(), MultinomialNB()),\n",
    "    'SVM': make_pipeline(TfidfVectorizer(), SVC(probability=True)),\n",
    "    'Logistic Regression': make_pipeline(TfidfVectorizer(), LogisticRegression())\n",
    "}\n",
    "\n",
    "# Train and save each model in the specified directory\n",
    "for name, model in models.items():\n",
    "    model.fit(data.data, data.target)\n",
    "    model_path = os.path.join(model_dir, f'{name.lower().replace(\" \", \"_\")}_classifier.pkl')\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction from Naive Bayes: rec.motorcycles\n",
      "Prediction from SVM: rec.motorcycles\n",
      "Prediction from Logistic Regression: rec.motorcycles\n",
      "Prediction from Naive Bayes: soc.religion.christian\n",
      "Prediction from SVM: sci.med\n",
      "Prediction from Logistic Regression: sci.med\n"
     ]
    }
   ],
   "source": [
    "# Define the directory where models are saved\n",
    "model_dir = r'C:\\Users\\ganad\\Desktop\\Resume Work\\NLP_HUB\\models'\n",
    "\n",
    "# Dictionary to map model names to file names\n",
    "model_files = {\n",
    "    'Naive Bayes': os.path.join(model_dir, 'naive_bayes_classifier.pkl'),\n",
    "    'SVM': os.path.join(model_dir, 'svm_classifier.pkl'),\n",
    "    'Logistic Regression': os.path.join(model_dir, 'logistic_regression_classifier.pkl')\n",
    "}\n",
    "\n",
    "# Function to load a model\n",
    "def load_model(model_name):\n",
    "    with open(model_files[model_name], 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    return model\n",
    "\n",
    "# Load the dataset to fetch categories\n",
    "data = fetch_20newsgroups(subset='train')\n",
    "categories = data.target_names\n",
    "\n",
    "# Load all models\n",
    "models = {name: load_model(name) for name in model_files.keys()}\n",
    "\n",
    "\n",
    "# Print predictions from each model\n",
    "for name, model in models.items():\n",
    "    prediction = model.predict([sentence])\n",
    "    print(f\"Prediction from {name}: {categories[prediction[0]]}\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    prediction = model.predict([complex_sentence])\n",
    "    print(f\"Prediction from {name}: {categories[prediction[0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similarity Measures**\n",
    "- Jaccard Index or Jaccard Similarity:\n",
    "    - Jaccard Similarity is the size of the intersection divided by the size of the union of two sets.\n",
    "    - Jaccard Similarity = (the number in both sets) / (the number in either set) * 100.\n",
    "    - Jaccard Similarity = (Intersection of A and B) / (Union of A and B) * 100.\n",
    "- Cosine Similarity:\n",
    "    - Cosine similarity calculates similarity by measuring the cosine of angle between two vectors.\n",
    "    - Cosine Similarity = (A . B) / (||A|| ||B||).\n",
    "    - Cosine Similarity = (Sum of A[i] * B[i]) / (Square Root of Sum of A[i]^2) * (Square Root of Sum of B[i]^2).\n",
    "- Euclidean Distance:\n",
    "    - Euclidean distance calculates the distance between two (or multiple) points.\n",
    "    - Euclidean Distance = sqrt(sum((x - y)^2))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(x,y):\n",
    "  \"\"\" returns the jaccard similarity between two lists \"\"\"\n",
    "  intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
    "  union_cardinality = len(set.union(*[set(x), set(y)]))\n",
    "  return intersection_cardinality/float(union_cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7777777777777778"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"Mcdonalds is unhealthy because it has processed food\",\n",
    "\"Denny's is unhealthy because it has processed food\"]\n",
    "sentences = [sent.lower().split(\" \") for sent in sentences]\n",
    "jaccard_similarity(sentences[0], sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "sentences = [\"Mcdonalds is unhealthy because it has processed food\", \"Mcdonalds is unhealthy because it has processed food\"]\n",
    "\n",
    "# Generate embeddings for each sentence\n",
    "embeddings = [nlp(sentence).vector for sentence in sentences]\n",
    "\n",
    "# Calculate the Euclidean distance between the first two embeddings\n",
    "distance = euclidean(embeddings[0], embeddings[1])\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import exp\n",
    "def distance_to_similarity(distance):\n",
    "  return 1/exp(distance)\n",
    "\n",
    "distance_to_similarity(distance) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9957017397069031\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "sentences = [\"Denny is unhealthy because it has processed food\", \"Mcdonalds is unhealthy because it has processed food\"]\n",
    "embeddings = [nlp(sentence).vector for sentence in sentences]\n",
    "\n",
    "cosine_similarity = 1 - cosine(embeddings[0], embeddings[1])\n",
    "print(cosine_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting rake_nltk\n",
      "  Downloading rake_nltk-1.0.6-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.6.2 in c:\\users\\ganad\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from rake_nltk) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\ganad\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\ganad\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ganad\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (2024.5.10)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ganad\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\ganad\\anaconda3\\envs\\nlpenv\\lib\\site-packages (from click->nltk<4.0.0,>=3.6.2->rake_nltk) (0.4.6)\n",
      "Downloading rake_nltk-1.0.6-py3-none-any.whl (9.1 kB)\n",
      "Installing collected packages: rake_nltk\n",
      "Successfully installed rake_nltk-1.0.6\n"
     ]
    }
   ],
   "source": [
    "!pip install rake_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.0: significantly improve patient outcomes\n",
      "9.0: enabling predictive analytics\n",
      "9.0: efficient diagnostic tools\n",
      "4.0: transforming healthcare\n",
      "4.0: personalized medicine\n",
      "4.0: operational efficiencies\n",
      "4.0: machine learning\n",
      "4.0: artificial intelligence\n"
     ]
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "rake = Rake()\n",
    "text = \"Artificial intelligence and machine learning are transforming healthcare by enabling predictive analytics, personalized medicine, and efficient diagnostic tools, which significantly improve patient outcomes and operational efficiencies.\"\n",
    "rake.extract_keywords_from_text(text)\n",
    "keywords = rake.get_ranked_phrases_with_scores()\n",
    "for score, phrase in keywords:\n",
    "    print(f\"{score}: {phrase}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets try using a job post as an example**\n",
    "https://g.co/kgs/pvxFeR9\n",
    "\n",
    "Machine Learning Engineer, Applied Machine\n",
    "\n",
    "Job highlights\n",
    "Identified by Google from the original job post\n",
    "Qualifications\n",
    "•\n",
    "Solid understanding of machine learning, deep learning (including LLMs) and natural language processing\n",
    "•\n",
    "5+ years proven programming skills using standard ML tools such as C/C++, Python, PyTorch, Tensorflow, HuggingFace, etc\n",
    "•\n",
    "Hands-on experience working (training, fine-tuning, optimizing, deploying) with large language models\n",
    "•\n",
    "Hands-on experience applying common machine learning optimization techniques, like quantization and distillation, to reduce the resource consumption and/or eliminate latency\n",
    "•\n",
    "Bachelors or MS in Computer Science or equivalent with Artificial Intelligence, Machine Learning, Data Science or related field\n",
    "Responsibilities\n",
    "•\n",
    "You will help design and implement our machine learning strategy to take our developer experience platform to the next level and help accelerate app development inside Apple\n",
    "•\n",
    "We will be collaborating and working with multi-functional teams and applying algorithms to large-scale data\n",
    "•\n",
    "You will work data scientist, Machine Learning engineers, Software engineers to deliver and end to end AI enabled solution for this platform\n",
    "•\n",
    "You will work with existing and new model evaluate them, fine tune them come up with use cases to solve business problems\n",
    "Benefits\n",
    "•\n",
    "Pay & Benefits\n",
    "•\n",
    "At Apple, base pay is one part of our total compensation package and is determined within a range\n",
    "•\n",
    "The base pay range for this role is between $138,900.00 and $256,500.00, and your base pay will depend on your skills, qualifications, experience, and location\n",
    "•\n",
    "You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses — including tuition\n",
    "•\n",
    "Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.0: years proven programming skills using standard ml tools\n",
      "36.06666666666666: experience applying common machine learning optimization techniques\n",
      "34.0: help accelerate app development inside apple\n",
      "14.5: end ai enabled solution\n",
      "9.566666666666666: machine learning strategy\n",
      "9.066666666666666: machine learning engineers\n",
      "9.0: solve business problems\n",
      "9.0: related field responsibilities\n",
      "9.0: qualifications solid understanding\n",
      "9.0: new model evaluate\n",
      "9.0: natural language processing\n",
      "9.0: developer experience platform\n",
      "8.0: large language models\n",
      "8.0: c ++, python\n",
      "7.333333333333334: work data scientist\n",
      "6.566666666666666: machine learning\n",
      "6.566666666666666: machine learning\n",
      "6.5: applying algorithms\n",
      "6.0: help design\n",
      "5.5: experience working\n",
      "5.166666666666666: deep learning\n",
      "4.5: software engineers\n",
      "4.333333333333334: scale data\n",
      "4.333333333333334: data science\n",
      "4.0: use cases\n",
      "4.0: resource consumption\n",
      "4.0: next level\n",
      "4.0: like quantization\n",
      "4.0: including llms\n",
      "4.0: functional teams\n",
      "4.0: eliminate latency\n",
      "4.0: computer science\n",
      "4.0: artificial intelligence\n",
      "3.5: fine tune\n",
      "2.5: end\n",
      "2.0: work\n",
      "2.0: platform\n",
      "2.0: large\n",
      "2.0: c\n",
      "1.5: working\n",
      "1.5: fine\n",
      "1.0: tuning\n",
      "1.0: training\n",
      "1.0: tensorflow\n",
      "1.0: take\n",
      "1.0: reduce\n",
      "1.0: pytorch\n",
      "1.0: optimizing\n",
      "1.0: multi\n",
      "1.0: ms\n",
      "1.0: implement\n",
      "1.0: huggingface\n",
      "1.0: hands\n",
      "1.0: hands\n",
      "1.0: existing\n",
      "1.0: etc\n",
      "1.0: equivalent\n",
      "1.0: distillation\n",
      "1.0: deploying\n",
      "1.0: deliver\n",
      "1.0: come\n",
      "1.0: collaborating\n",
      "1.0: bachelors\n",
      "1.0: 5\n"
     ]
    }
   ],
   "source": [
    "text = \"Qualifications Solid understanding of machine learning, deep learning (including LLMs) and natural language processing. 5+ years proven programming skills using standard ML tools such as C/C++, Python, PyTorch, Tensorflow, HuggingFace, etc. Hands-on experience working (training, fine-tuning, optimizing, deploying) with large language models. Hands-on experience applying common machine learning optimization techniques, like quantization and distillation, to reduce the resource consumption and/or eliminate latency. Bachelors or MS in Computer Science or equivalent with Artificial Intelligence, Machine Learning, Data Science or related field Responsibilities. You will help design and implement our machine learning strategy to take our developer experience platform to the next level and help accelerate app development inside Apple. We will be collaborating and working with multi-functional teams and applying algorithms to large-scale data. You will work data scientist, Machine Learning engineers, Software engineers to deliver and end to end AI enabled solution for this platform. You will work with existing and new model evaluate them, fine tune them come up with use cases to solve business problems\"\n",
    "\n",
    "rake.extract_keywords_from_text(text)\n",
    "keywords = rake.get_ranked_phrases_with_scores()\n",
    "for score, phrase in keywords:\n",
    "    print(f\"{score}: {phrase}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Topics:\n",
      "Topic 1:\n",
      "[('improving', 1.4974504780382687), ('diagnostic', 1.4974504780382687), ('patient', 1.4974504780382687), ('outcomes', 1.4974504780382687), ('operational', 1.4974504780382687), ('tools', 1.4974504780382687), ('efficiencies', 1.4974504780382687), ('efficient', 1.4974504780382687), ('prevalent', 1.4958830831626244), ('medicine', 1.495883083162623)]\n",
      "\n",
      "\n",
      "Topic 2:\n",
      "[('intelligence', 2.4967267443510845), ('artificial', 2.496726744293454), ('healthcare', 2.4967267442656587), ('adopting', 1.497264948062675), ('systems', 1.497264948062675), ('better', 1.497264948062675), ('performance', 1.497264948062675), ('increasingly', 1.497264948062675), ('learning', 1.4963600003121), ('transforming', 1.4963600001151094)]\n",
      "\n",
      "\n",
      "LSA Topics:\n",
      "Topic 1:\n",
      "[('artificial', 0.40478509903979104), ('healthcare', 0.40478509903979104), ('intelligence', 0.40478509903979104), ('transforming', 0.27875639219256004), ('machine', 0.27875639219256004), ('learning', 0.27875639219256004), ('adopting', 0.2346621794075062), ('increasingly', 0.234662179407506), ('systems', 0.234662179407506), ('better', 0.234662179407506)]\n",
      "\n",
      "\n",
      "Topic 2:\n",
      "[('analytics', 0.44721359549995826), ('prevalent', 0.44721359549995804), ('predictive', 0.44721359549995804), ('personalized', 0.44721359549995804), ('medicine', 0.44721359549995804), ('systems', 1.249000902703301e-16), ('performance', 1.249000902703301e-16), ('better', 1.1102230246251565e-16), ('artificial', 1.0755285551056204e-16), ('increasingly', 6.938893903907228e-17)]\n",
      "\n",
      "\n",
      "LDA Document-Topic Distributions:\n",
      "[[0.07379132 0.92620868]\n",
      " [0.91322445 0.08677555]\n",
      " [0.94217713 0.05782287]\n",
      " [0.05755097 0.94244903]]\n",
      "LSA Document-Topic Distributions:\n",
      "[[ 8.13230495e-01 -2.06630340e-17]\n",
      " [ 1.05507702e-16  1.00000000e+00]\n",
      " [-1.12850395e-16 -3.14018492e-16]\n",
      " [ 8.13230495e-01  1.73287394e-16]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "\n",
    "# Sample clean corpus\n",
    "clean_corpus = [\n",
    "    'artificial intelligence and machine learning are transforming healthcare',\n",
    "    'predictive analytics and personalized medicine are becoming more prevalent',\n",
    "    'efficient diagnostic tools and operational efficiencies are improving patient outcomes',\n",
    "    'healthcare systems are increasingly adopting artificial intelligence for better performance'\n",
    "]\n",
    "\n",
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {idx + 1}:\")\n",
    "        print([(vectorizer.get_feature_names_out()[i], topic[i]) for i in topic.argsort()[:-top_n - 1:-1]])\n",
    "        print(\"\\n\")\n",
    "\n",
    "# LDA \n",
    "vectorizer_lda = CountVectorizer(stop_words='english')\n",
    "doc_term_matrix_lda = vectorizer_lda.fit_transform(clean_corpus)\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=2, random_state=42)\n",
    "lda_model.fit(doc_term_matrix_lda)\n",
    "\n",
    "print(\"LDA Topics:\")\n",
    "print_topics(lda_model, vectorizer_lda)\n",
    "\n",
    "# LSA \n",
    "vectorizer_lsa = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix_lsa = vectorizer_lsa.fit_transform(clean_corpus)\n",
    "\n",
    "lsa_model = TruncatedSVD(n_components=2, random_state=42)\n",
    "lsa_model.fit(tfidf_matrix_lsa)\n",
    "\n",
    "print(\"LSA Topics:\")\n",
    "print_topics(lsa_model, vectorizer_lsa)\n",
    "\n",
    "# Document-Topic Distributions for LDA\n",
    "doc_topic_distributions_lda = lda_model.transform(doc_term_matrix_lda)\n",
    "print(\"LDA Document-Topic Distributions:\")\n",
    "print(doc_topic_distributions_lda)\n",
    "\n",
    "# Document-Topic Distributions for LSA\n",
    "doc_topic_distributions_lsa = lsa_model.transform(tfidf_matrix_lsa)\n",
    "print(\"LSA Document-Topic Distributions:\")\n",
    "print(doc_topic_distributions_lsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ganad\\anaconda3\\envs\\nlpenv\\lib\\site-packages\n",
      "Original Document Size: 1820\n",
      "Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, \n",
      "recurrent neural networks and convolutional neural networks have been applied to\n",
      "fields including computer vision, speech recognition, natural language processing, \n",
      "machine translation, bioinformatics, drug design, medical image analysis, material\n",
      "inspection and board game programs, where they have produced results comparable to \n",
      "and in some cases surpassing human expert performance.\n",
      "Summary Length: 81\n",
      "The adjective \"deep\" in deep learning refers to the use of multiple\n",
      "layers in the network.\n",
      "Summary Length: 20\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pytextrank\n",
    " \n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.add_pipe(\"textrank\")\n",
    " \n",
    "example_text = \"\"\"Deep learning (also known as deep structured learning) is part of a \n",
    "broader family of machine learning methods based on artificial neural networks with \n",
    "representation learning. Learning can be supervised, semi-supervised or unsupervised. \n",
    "Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, \n",
    "recurrent neural networks and convolutional neural networks have been applied to\n",
    "fields including computer vision, speech recognition, natural language processing, \n",
    "machine translation, bioinformatics, drug design, medical image analysis, material\n",
    "inspection and board game programs, where they have produced results comparable to \n",
    "and in some cases surpassing human expert performance. Artificial neural networks\n",
    "(ANNs) were inspired by information processing and distributed communication nodes\n",
    "in biological systems. ANNs have various differences from biological brains. Specifically, \n",
    "neural networks tend to be static and symbolic, while the biological brain of most living organisms\n",
    "is dynamic (plastic) and analogue. The adjective \"deep\" in deep learning refers to the use of multiple\n",
    "layers in the network. Early work showed that a linear perceptron cannot be a universal classifier, \n",
    "but that a network with a nonpolynomial activation function with one hidden layer of unbounded width can.\n",
    "Deep learning is a modern variation which is concerned with an unbounded number of layers of bounded size, \n",
    "which permits practical application and optimized implementation, while retaining theoretical universality \n",
    "under mild conditions. In deep learning the layers are also permitted to be heterogeneous and to deviate widely \n",
    "from biologically informed connectionist models, for the sake of efficiency, trainability and understandability, \n",
    "whence the structured part.\"\"\"\n",
    "print('Original Document Size:',len(example_text))\n",
    "doc = nlp(example_text)\n",
    " \n",
    "for sent in doc._.textrank.summary(limit_phrases=2, limit_sentences=2):\n",
    "    print(sent)\n",
    "    print('Summary Length:',len(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "to_translate = 'I like to live my life'\n",
    "translated = GoogleTranslator(source='auto', target='ar').translate(to_translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "أحب أن أعيش حياتي\n"
     ]
    }
   ],
   "source": [
    "print(translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_translate = 'I love you'\n",
    "translated = GoogleTranslator(source='auto', target='fr').translate(to_translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je t'aime\n"
     ]
    }
   ],
   "source": [
    "print(translated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I know English, Arabic and a bit of French and that is why I chose this example**\n",
    "\n",
    "Both are Correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "- Bird, Steven, Edward Loper and Ewan Klein (2009).\n",
    "Natural Language Processing with Python.  O'Reilly Media Inc.\n",
    "- https://neptune.ai/blog/tokenization-in-nlp\n",
    "- https://www.sketchengine.eu/penn-treebank-tagset/\n",
    "- https://www.newscatcherapi.com/blog/ultimate-guide-to-text-similarity-with-python#toc-3\n",
    "- https://towardsdatascience.com/keyword-extraction-process-in-python-with-natural-language-processing-nlp-d769a9069d5c\n",
    "- https://www.datacamp.com/tutorial/what-is-topic-modeling\n",
    "- https://www.geeksforgeeks.org/text-summarization-in-nlp/\n",
    "- https://medium.com/analytics-vidhya/how-to-translate-text-with-python-9d203139dcf5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
